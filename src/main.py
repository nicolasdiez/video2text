# /src/main.py

# TODO:
# - implementar “Context‑Aware Tweet Generation” con RAG (ANTES de generar tweets, lanzar semantic query a vectorDB y pasar los 10 tweets similares como contexto en el prompt. DESPUES de generar tweets --> embedding_model(tweet) --> persist vector in vectorDB)
# - implementar "RAG para “Tweet Style Optimization” basado en rendimiento real"
# - change naming of entidad "Prompt" por "UserPrompt" (channel.selected_prompt_id)

# - en main, separar la condicion is_running de enough_time_passed
# - en los routers para que el usuario asigne prompts a sus channels --> usar los metodos del prompt_service.py (ej. borrar prompt, update...)
# - en los routers para que el usuario haga cambios en sus channels --> usar los metodos del channel_service.py (ej. update_channel_prompt()...)

# - cuando un service A empiece a usar PromptService, inyectar PromptService en A (builder) desde main.py (composition root)
# - cuando un service A empiece a usar ChannelService, inyectar ChannelService en A (builder) desde main.py (composition root)
# - endpoints de consumo desde front para CRUD entities: users, channels, prompts, app_config, prompts_master.
# - modify transcription_client.py from using deprecated get_transcript() to use fetch()
# - create a new collection {prompts_master} to store master prompts of the application, not dependent on userId or channelId.
# - refactor ingestion_pipeline_service constructor to use a Composite pattern for the transcription clients/adapters (crear un CompositeTranscriptionClient que reciba [primary, fallback1, fallback2...] y pruebe cada uno en orden hasta obtener resultado válido. Mantiene Inversion of Control y SRP.)
# - in GCP VM, convert./run.sh into a persistent service, so it runs in background all time, not foreground execution needed anymore
# - create a new feature/flag on channel entity to request user approval for tweets generated by a channel --> channel.isUserApprovalNeededToPublish + tweet.isUserApprovalNeededToPublish + tweet.isPublishingApprovedByUser

import os
import asyncio
import sys

# import config
import config

# set twitter credentials for user Nico (TEMPORATY: UNTIL API AND FRONTEND READY) 
from domain.entities.user import UserTwitterCredentials

# logger
import logging
import inspect
from infrastructure.logging.request_context import set_user_id

# WebServer
import uvicorn      # ASGI ligero y de alto rendimiento (Asynchronous Server Gateway Interface server)

# Fast API framework
from fastapi import FastAPI

# Mongo DB
from infrastructure.mongodb import db

# APScheduler
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from contextlib import asynccontextmanager
from datetime import datetime, timedelta, timezone

# Controllers
# import pipeline_controller to later inject the IngestionPipelineService/PublishingPipelineService instances with all the created adapters into pipeline_controller.ingestion_pipeline_service/publishing_pipeline_service
import adapters.inbound.http.pipeline_controller as pipeline_controller 

# Ingestion pipeline
from application.services.ingestion_pipeline_service import IngestionPipelineService
from adapters.outbound.mongodb.user_repository import MongoUserRepository
from adapters.outbound.file_prompt_loader import FilePromptLoader
from adapters.outbound.mongodb.channel_repository import MongoChannelRepository
from adapters.outbound.youtube_video_client import YouTubeVideoClient
from adapters.outbound.mongodb.video_repository import MongoVideoRepository
from adapters.outbound.transcription_client_captions_api import YouTubeTranscriptionClientOfficialCaptionsAPI
from adapters.outbound.transcription_client_data_api import YouTubeTranscriptionClientOfficialDataAPI
from adapters.outbound.transcription_client_public_player_api_ASR import YouTubeTranscriptionClientOfficialPublicPlayerAPI_ASR
from adapters.outbound.transcription_client_android_player_api_ASR import YouTubeTranscriptionClientAndroidPlayerAPI_ASR
from adapters.outbound.mongodb.user_prompt_repository import MongoPromptRepository
from adapters.outbound.llm_openai_client import LLMOpenAIClient
from adapters.outbound.mongodb.tweet_generation_repository import MongoTweetGenerationRepository
from adapters.outbound.mongodb.tweet_repository import MongoTweetRepository
from adapters.outbound.mongodb.user_scheduler_runtime_status_repository import MongoUserSchedulerRuntimeStatusRepository

# Publishing pipeline
from application.services.publishing_pipeline_service import PublishingPipelineService
from adapters.outbound.twitter_publication_client import TwitterPublicationClient

# Stats pipeline
from application.services.stats_pipeline_service import StatsPipelineService
from adapters.outbound.twitter_stats.twitter_stats_client_apify_apidojo_tweet_scraper import TwitterStatsClientApifyApidojoTweetScraper
from domain.services.growth_score_calculator_service import GrowthScoreCalculatorService

# Embeddings pipeline
from application.services.embeddings_pipeline_service import EmbeddingsPipelineService
from adapters.outbound.mongodb.embedding_vector_repository import MongoEmbeddingVectorRepository
from adapters.outbound.embedding_vector_openai_client import EmbeddingVectorOpenAIClient

# Repository adapters (for wiring with DB instance)
from adapters.outbound.mongodb.app_config_repository import MongoAppConfigRepository
from adapters.outbound.mongodb.master_prompt_repository import MongoMasterPromptRepository

# Application Services ()
from application.services.channel_service import ChannelService
from application.services.prompt_composer_service import PromptComposerService
from application.services.tweet_outpout_guardrail_service import TweetOutputGuardrailService

# factory to get a youtube_client resource for consuming Youtube Data API (to retrieve video transcriptions) 
from infrastructure.auth.youtube_credentials import get_youtube_client

# specific logger for this module
logger = logging.getLogger(__name__)

# create a youtube_client resource to inject as dependency into YouTubeTranscriptionClientOfficialDataAPI
try:
    youtube_client = get_youtube_client(client_id=config.YOUTUBE_OAUTH_CLIENT_ID, client_secret=config.YOUTUBE_OAUTH_CLIENT_SECRET, refresh_token=config.YOUTUBE_OAUTH_CLIENT_REFRESH_TOKEN)
except RuntimeError as exc:
    logger.error("YouTube client could not be constructed: %s", str(exc), extra={"mod": __name__})
    youtube_client = None

# --- Repo adapters & service instantiation ---

# Ingestion, Publishing, Stats, Embeddings pipelines 
user_repo                                   = MongoUserRepository(database=db)
prompt_loader                               = FilePromptLoader(prompts_dir="prompts")
channel_repo                                = MongoChannelRepository(database=db)
video_source                                = YouTubeVideoClient(api_key=config.YOUTUBE_API_KEY)
video_repo                                  = MongoVideoRepository(database=db)
transcription_client_captions_api           = YouTubeTranscriptionClientOfficialCaptionsAPI(default_language="es")
transcription_client_data_api               = YouTubeTranscriptionClientOfficialDataAPI(youtube_client=youtube_client) if youtube_client else None
transcription_client_public_player_api_asr  = YouTubeTranscriptionClientOfficialPublicPlayerAPI_ASR(model_name="tiny", device="cpu")
transcription_client_android_player_api_asr = YouTubeTranscriptionClientAndroidPlayerAPI_ASR(model_name="small", device="cpu")
prompt_repo                                 = MongoPromptRepository(database=db)
openai_client                               = LLMOpenAIClient(api_key=config.OPENAI_API_KEY)
tweet_output_guardrail_service              = TweetOutputGuardrailService()
tweet_generation_repo                       = MongoTweetGenerationRepository(db=db)
tweet_repo                                  = MongoTweetRepository(database=db)
user_scheduler_runtime_repo                 = MongoUserSchedulerRuntimeStatusRepository(database=db)
master_prompt_repo                          = MongoMasterPromptRepository(database=db) 
channel_service                             = ChannelService(channel_repo, prompt_repo, master_prompt_repo)
prompt_composer_service                     = PromptComposerService()

# Create an instance of IngestionPipelineService with the concrete implementations of the ports (i.e., inject Adapters into the Ports of IngestionPipelineService)
ingestion_pipeline_service_instance = IngestionPipelineService(
    user_repo                       = user_repo,
    prompt_loader                   = prompt_loader,
    channel_repo                    = channel_repo,
    video_source                    = video_source,
    video_repo                      = video_repo,
    transcription_client            = transcription_client_captions_api,
    transcription_client_fallback   = transcription_client_public_player_api_asr,
    transcription_client_fallback_2 = transcription_client_data_api,
    openai_client                   = openai_client,
    tweet_output_guardrail_service  = tweet_output_guardrail_service,
    tweet_generation_repo           = tweet_generation_repo,
    tweet_repo                      = tweet_repo,
    user_scheduler_runtime_repo     = user_scheduler_runtime_repo,
    channel_service                 = channel_service,
    prompt_composer_service         = prompt_composer_service
)

# Inject the instance of IngestionPipelineService (with all the Adapters) into the pipeline controller 
pipeline_controller.ingestion_pipeline_service = ingestion_pipeline_service_instance


# --- Publishing adapters & service instantiation ---
twitter_publication_client  = TwitterPublicationClient(
    oauth1_api_key          = config.X_OAUTH1_API_KEY,
    oauth1_api_secret       = config.X_OAUTH1_API_SECRET
)

# Create an instance of PublishingPipelineService with the concrete implementations of the ports (i.e., inject Adapters into the Ports of PublishingPipelineService)
publishing_pipeline_service_instance = PublishingPipelineService(
    user_repo                       = user_repo,
    tweet_repo                      = tweet_repo,
    twitter_publication_client      = twitter_publication_client,
    user_scheduler_runtime_repo     = user_scheduler_runtime_repo,
)

# Inject the instance of PublishingPipelineService (with all the Adapters) into the pipeline controller 
pipeline_controller.publishing_pipeline_service = publishing_pipeline_service_instance

# Stats pipeline
stats_provider = TwitterStatsClientApifyApidojoTweetScraper(apify_token=config.APIFY_API_TOKEN_PERSONAL)
growth_score_calculator = GrowthScoreCalculatorService()

stats_pipeline_service = StatsPipelineService(
    user_repo                   = user_repo,
    tweet_repo                  = tweet_repo,
    stats_provider              = stats_provider,
    growth_score_calculator     = growth_score_calculator,
    user_scheduler_runtime_repo = user_scheduler_runtime_repo,
)

# Embeddings pipeline
embeddings_repo = MongoEmbeddingVectorRepository(database=db)
embeddings_client = EmbeddingVectorOpenAIClient(api_key=config.OPENAI_API_KEY, base_url = "https://api.openai.com/v1")

embeddings_pipeline_servive = EmbeddingsPipelineService(
    user_repo                                   = user_repo,
    tweet_repo                                  = tweet_repo,
    video_repo                                  = video_repo,
    embedding_repo                              = embeddings_repo,
    embeddings_client                           = embeddings_client,
    user_scheduler_runtime_repo                 = user_scheduler_runtime_repo,
    embedding_model                             = "text-embedding-3-small",
    tweet_max_days_back_calculate_embeddings    = 60,
)

# --- AppConfig adapter ---
app_config_repo = MongoAppConfigRepository(database=db)

# APScheduler instance
scheduler = AsyncIOScheduler()


# Lifespan context manager (replaces deprecated @app.on_event)
@asynccontextmanager
async def lifespan(app: FastAPI):

    # ===== START TEMPORARY BLOCK =====
    # Escribir en el document del USER_ID las credentials de usuario que temporalmente están en .env
    # TODO: remove this block when frontend/endpoints for user credential management is ready
    USER_ID = "000000000000000000000001" # Nico
    bootstrap_user_id = USER_ID
    # Retrieve USER's X credentials from env (either .env file or Github Environment secrets) and save them encrypted to mongoDB user collection
    creds = UserTwitterCredentials(
        # credentials related to THE USER of the application:
        oauth1_access_token=config.X_OAUTH1_ACCESS_TOKEN,
        oauth1_access_token_secret=config.X_OAUTH1_ACCESS_TOKEN_SECRET,
        oauth2_access_token=config.X_OAUTH2_ACCESS_TOKEN,
        oauth2_access_token_expires_at=config.X_OAUTH2_ACCESS_TOKEN_EXPIRES_AT,
        oauth2_refresh_token=config.X_OAUTH2_REFRESH_TOKEN,
        oauth2_refresh_token_expires_at=config.X_OAUTH2_REFRESH_TOKEN_EXPIRES_AT,
        screen_name=config.X_SCREEN_NAME
    )
    await user_repo.update_twitter_credentials(bootstrap_user_id, creds)
    logger.info("TEMPORARY --> Twitter user credentials written in MongoDB for bootstrap user: %s", bootstrap_user_id)
    # ===== END TEMPORARY BLOCK =====


    # ===== Inline async function for INGESTION PIPELINE =====
    async def ingestion_job():
        # 1. Get pipeline execution frequency at app config level
        app_config = await app_config_repo.get_config()
        # app_frequency_minutes = float(app_config.scheduler_config.ingestion_pipeline_frequency_minutes)
        default_user_frequency_minutes = 1440

        users = await user_repo.find_all()
        now = datetime.utcnow()

        for user in users:
            try:
                # Set user_id for this iteration to make it available for logging
                set_user_id(str(user.id))

                # 2. Check if pipeline is enabled (user config takes priority, then app config)
                user_scheduler_config = getattr(user, "scheduler_config", None)
                if user_scheduler_config and hasattr(user_scheduler_config, "is_ingestion_pipeline_enabled") and user_scheduler_config.is_ingestion_pipeline_enabled is False:
                    logger.info("Skipping Ingestion pipeline (disabled by user config)", extra={"job": "ingestion"})
                    continue

                app_scheduler_config = app_config.scheduler_config
                if not app_scheduler_config or not hasattr(app_scheduler_config, "is_ingestion_pipeline_enabled") or app_scheduler_config.is_ingestion_pipeline_enabled is False:
                    logger.info("Skipping Ingestion pipeline (disabled by app_config or app_config missing)", extra={"job": "ingestion"})
                    continue

                # 3. Determine effective pipeline frequency (user config takes priority, then app config)
                user_frequency_minutes = getattr(user.scheduler_config, "ingestion_pipeline_frequency_minutes", None)
                effective_frequency_minutes = float(user_frequency_minutes) if user_frequency_minutes is not None else default_user_frequency_minutes #app_frequency_minutes

                # 4. Retrieve runtime status for this user
                user_runtime_status = await user_scheduler_runtime_repo.get_by_user_id(user.id)
                ingestion_last_started_at = getattr(user_runtime_status, "last_ingestion_pipeline_started_at", None) if user_runtime_status else None
                is_running = getattr(user_runtime_status, "is_ingestion_pipeline_running", False) if user_runtime_status else False

                # 5. Determine if pipeline should run
                elapsed_minutes = (now - ingestion_last_started_at).total_seconds() / 60.0 if ingestion_last_started_at else None
                # normal condition: enough time has passed AND pipeline is not running
                enough_time_passed = elapsed_minutes is not None and elapsed_minutes > effective_frequency_minutes and not is_running
                # protection condition: pipeline stuck (elapsed > 1x frequency)
                stuck_protection = elapsed_minutes is not None and elapsed_minutes > (effective_frequency_minutes * 1)
                # first run condition: no previous execution recorded 
                first_run = elapsed_minutes is None
                
                should_run = first_run or enough_time_passed or stuck_protection
                
                # normalizo valor de elapsed_minutes para el caso de que sea None no falle el logger
                elapsed_minutes = f"{elapsed_minutes:.2f}" if elapsed_minutes is not None else "N/A"
                decision = "Yes" if should_run else "No"
                logger.info("Ingestion pipeline: Configured freq %s mins, Last start %s mins ago", effective_frequency_minutes, elapsed_minutes, extra={"job": "ingestion"})
                logger.info("Ingestion pipeline should run now? %s", decision, extra={"job": "ingestion"})

                if not should_run:
                    logger.info("Skipping Ingestion pipeline (already running or within freq)", extra={"job": "ingestion"})
                    continue

                # 6. Run pipeline
                logger.info("Ingestion pipeline starting", extra={"job": "ingestion"})
                set_user_id(user.id) # se aplica a todos los logs del job
                await ingestion_pipeline_service_instance.run_for_user(user_id=user.id)
                logger.info("Ingestion pipeline finished", extra={"job": "ingestion"})
                
                # 7. Update the time for next pipeline initiation using the effective frequency (user or app)
                finish_time = datetime.utcnow()
                next_start = finish_time + timedelta(minutes=effective_frequency_minutes)
                await user_scheduler_runtime_repo.update_by_user_id(user.id, {"nextScheduledIngestionPipelineStartingAt": next_start})
                logger.info("Next user's scheduled Ingestion pipeline starting at: %s", next_start.isoformat(), extra={"job": "ingestion"})
            
            except Exception as e:
                logger.error("Ingestion pipeline failed: %s", str(e), extra={"job": "ingestion"})
    
        # 8. Refresh app config from repository and reschedule job if frequency changed
        # get current app job/pipeline frequency
        job = scheduler.get_job("ingestion_job")
        current_ingestion_frequency_minutes = job.trigger.interval.total_seconds() / 60
        logger.debug("Checking if Ingestion pipeline app config frequency has changed (current freq: %s mins)", current_ingestion_frequency_minutes, extra={"job": "ingestion"})
        # get app configuration frequency from repo
        new_app_config = await app_config_repo.get_config()
        new_ingestion_frequency_minutes = new_app_config.scheduler_config.ingestion_pipeline_frequency_minutes
        # if there is a new app pipeline config then reschedule job
        if float(new_ingestion_frequency_minutes) != float(current_ingestion_frequency_minutes):
            try:
                scheduler.reschedule_job("ingestion_job", trigger="interval", minutes=new_ingestion_frequency_minutes)
                logger.info("Rescheduled Ingestion pipeline app config frequency to %s minutes", new_ingestion_frequency_minutes, extra={"job": "ingestion"})
            except Exception as ex:
                logger.warning("Failed to reschedule Ingestion pipeline app config frequency: %s", str(ex), extra={"job": "ingestion"})
        else:
            logger.debug("Ingestion pipeline app config frequency has not changed (current freq: %s mins)", current_ingestion_frequency_minutes, extra={"job": "ingestion"})



    # ===== Inline async function for PUBLISHING PIPELINE =====
    async def publishing_job():
        # 1. Get pipeline execution frequency at app config level
        app_config = await app_config_repo.get_config()
        # app_frequency_minutes = float(app_config.scheduler_config.publishing_pipeline_frequency_minutes)
        default_user_frequency_minutes = 1440

        users = await user_repo.find_all()
        now = datetime.utcnow()

        for user in users:
            try:
                # Set user_id for this iteration to make it available for logging
                set_user_id(str(user.id))
                
                # 2. Check if pipeline is enabled (user config takes priority, then app config)
                user_scheduler_config = getattr(user, "scheduler_config", None)
                if user_scheduler_config and hasattr(user_scheduler_config, "is_publishing_pipeline_enabled") and user_scheduler_config.is_publishing_pipeline_enabled is False:
                    logger.info("Skipping Publishing pipeline (disabled by user config)", extra={"job": "publishing"})
                    continue

                app_scheduler_config = app_config.scheduler_config
                if not app_scheduler_config or not hasattr(app_scheduler_config, "is_publishing_pipeline_enabled") or app_scheduler_config.is_publishing_pipeline_enabled is False:
                    logger.info("Skipping Publishing pipeline (disabled by app_config or app_config missing)", extra={"job": "publishing"})
                    continue

                # 3. Determine effective pipeline frequency (user config takes priority, then app config)
                user_frequency_minutes = getattr(user.scheduler_config, "publishing_pipeline_frequency_minutes", None)
                effective_frequency_minutes = float(user_frequency_minutes) if user_frequency_minutes is not None else default_user_frequency_minutes #app_frequency_minutes

                # 4. Retrieve runtime status for this user
                user_runtime_status = await user_scheduler_runtime_repo.get_by_user_id(user.id)
                publishing_last_started_at = getattr(user_runtime_status, "last_publishing_pipeline_started_at", None) if user_runtime_status else None
                is_running = getattr(user_runtime_status, "is_publishing_pipeline_running", False) if user_runtime_status else False

                # 5. Determine if pipeline should run
                elapsed_minutes = (now - publishing_last_started_at).total_seconds() / 60.0 if publishing_last_started_at else None
                # normal condition: enough time has passed AND pipeline is not running
                enough_time_passed = elapsed_minutes is not None and elapsed_minutes > effective_frequency_minutes and not is_running
                # protection condition: pipeline stuck (elapsed > 1x frequency)
                stuck_protection = elapsed_minutes is not None and elapsed_minutes > (effective_frequency_minutes * 1)
                # first run condition: no previous execution recorded 
                first_run = elapsed_minutes is None

                should_run = first_run or enough_time_passed or stuck_protection

                # normalizo valor de elapsed_minutes para el caso de que sea None no falle el logger
                elapsed_minutes = f"{elapsed_minutes:.2f}" if elapsed_minutes is not None else "N/A"
                decision = "Yes" if should_run else "No"
                logger.info("Publishing pipeline: Configured freq %s mins, Last start %s mins ago", effective_frequency_minutes, elapsed_minutes, extra={"job": "publishing"})
                logger.info("Publishing pipeline should run now? %s", decision, extra={"job": "publishing"})
                
                if not should_run:
                    logger.info("Skipping Publishing pipeline (already running or within freq)", extra={"job": "publishing"})
                    continue

                # 6. Run pipeline
                logger.info("Publishing pipeline starting", extra={"job": "publishing"})
                set_user_id(user.id) # se aplica a todos los logs del job
                await publishing_pipeline_service_instance.run_for_user(user_id=user.id)
                logger.info("Publishing pipeline finished", extra={"job": "publishing"})

                # 7. Update the time for next pipeline initiation using the effective frequency (user or app)
                finish_time = datetime.utcnow()
                next_start = finish_time + timedelta(minutes=effective_frequency_minutes)
                await user_scheduler_runtime_repo.update_by_user_id(user.id, {"nextScheduledPublishingPipelineStartingAt": next_start})
                logger.info("Next user's scheduled Publishing pipeline starting at: %s", next_start.isoformat(), extra={"job": "publishing"})

            except Exception as e:
                logger.error("Publishing pipeline failed: %s", str(e), extra={"job": "publishing"})

        # 8. Refresh app config from repository and reschedule job if frequency changed
        # get current app job/pipeline frequency
        job = scheduler.get_job("publishing_job")
        current_publishing_frequency_minutes = job.trigger.interval.total_seconds() / 60
        logger.debug("Checking if Publishing pipeline app config frequency has changed (current freq: %s mins)", current_publishing_frequency_minutes, extra={"job": "publishing"})
        # get app configuration frequency from repo
        new_app_config = await app_config_repo.get_config()
        new_publishing_frequency_minutes = new_app_config.scheduler_config.publishing_pipeline_frequency_minutes
        # if there is a new app pipeline config then reschedule job
        if float(new_publishing_frequency_minutes) != float(current_publishing_frequency_minutes):
            try:
                scheduler.reschedule_job("publishing_job", trigger="interval", minutes=new_publishing_frequency_minutes)
                logger.info("Rescheduled Publishing pipeline app config frequency to %s minutes", new_publishing_frequency_minutes, extra={"job": "publishing"})
            except Exception as ex:
                logger.warning("Failed to reschedule Publishing pipeline app config frequency: %s", str(ex), extra={"job": "publishing"})
        else:   
            logger.debug("Publishing pipeline app config frequency has not changed (current freq: %s mins)", current_publishing_frequency_minutes, extra={"job": "ingestion"})



    # ===== Inline async function for STATS PIPELINE =====
    async def stats_job():
        # 1. Get app-level config
        app_config = await app_config_repo.get_config()
        default_user_frequency_minutes = 1440

        users = await user_repo.find_all()
        now = datetime.utcnow()

        for user in users:
            try:
                set_user_id(str(user.id))

                # 2. Check if pipeline is enabled
                user_scheduler_config = getattr(user, "scheduler_config", None)
                if user_scheduler_config and hasattr(user_scheduler_config, "is_stats_pipeline_enabled") and user_scheduler_config.is_stats_pipeline_enabled is False:
                    logger.info("Skipping Stats pipeline (disabled by user config)", extra={"job": "stats"})
                    continue

                app_scheduler_config = app_config.scheduler_config
                if not app_scheduler_config or not hasattr(app_scheduler_config, "is_stats_pipeline_enabled") or app_scheduler_config.is_stats_pipeline_enabled is False:
                    logger.info("Skipping Stats pipeline (disabled by app_config or app_config missing)", extra={"job": "stats"})
                    continue

                # 3. Determine effective frequency
                user_frequency_minutes = getattr(user.scheduler_config, "stats_pipeline_frequency_minutes", None)
                effective_frequency_minutes = float(user_frequency_minutes) if user_frequency_minutes is not None else default_user_frequency_minutes

                # 4. Retrieve runtime status
                user_runtime_status = await user_scheduler_runtime_repo.get_by_user_id(user.id)
                stats_last_started_at = getattr(user_runtime_status, "last_stats_pipeline_started_at", None) if user_runtime_status else None
                is_running = getattr(user_runtime_status, "is_stats_pipeline_running", False) if user_runtime_status else False

                # 5. Determine if pipeline should run
                elapsed_minutes = (now - stats_last_started_at).total_seconds() / 60.0 if stats_last_started_at else None
                enough_time_passed = elapsed_minutes is not None and elapsed_minutes > effective_frequency_minutes and not is_running
                stuck_protection = elapsed_minutes is not None and elapsed_minutes > (effective_frequency_minutes * 1)
                first_run = elapsed_minutes is None

                should_run = first_run or enough_time_passed or stuck_protection

                elapsed_minutes_str = f"{elapsed_minutes:.2f}" if elapsed_minutes is not None else "N/A"
                decision = "Yes" if should_run else "No"

                logger.info("Stats pipeline: Configured freq %s mins, Last start %s mins ago", effective_frequency_minutes, elapsed_minutes_str, extra={"job": "stats"})
                logger.info("Stats pipeline should run now? %s", decision, extra={"job": "stats"})

                if not should_run:
                    logger.info("Skipping Stats pipeline (already running or within freq)", extra={"job": "stats"})
                    continue

                # 6. Run pipeline
                logger.info("Stats pipeline starting", extra={"job": "stats"})
                set_user_id(user.id)
                await stats_pipeline_service.run_for_user(user_id=user.id)
                logger.info("Stats pipeline finished", extra={"job": "stats"})

                # 7. Update next scheduled run
                finish_time = datetime.utcnow()
                next_start = finish_time + timedelta(minutes=effective_frequency_minutes)
                await user_scheduler_runtime_repo.update_by_user_id(user.id, {"nextScheduledStatsPipelineStartingAt": next_start})
                logger.info("Next user's scheduled Stats pipeline starting at: %s", next_start.isoformat(), extra={"job": "stats"})

            except Exception as e:
                logger.error("Stats pipeline failed: %s", str(e), extra={"job": "stats"})

        # 8. Refresh app config and reschedule job if needed
        # get current app job/pipeline frequency
        job = scheduler.get_job("stats_job")
        current_stats_frequency_minutes = job.trigger.interval.total_seconds() / 60
        logger.debug("Checking if Stats pipeline app config frequency has changed (current freq: %s mins)", current_stats_frequency_minutes, extra={"job": "stats"})
        # get app configuration frequency from repo
        new_app_config = await app_config_repo.get_config()
        new_stats_frequency_minutes = new_app_config.scheduler_config.stats_pipeline_frequency_minutes
        # if there is a new app pipeline config then reschedule job
        if float(new_stats_frequency_minutes) != float(current_stats_frequency_minutes):
            try:
                scheduler.reschedule_job("stats_job", trigger="interval", minutes=new_stats_frequency_minutes)
                logger.info("Rescheduled Stats pipeline app config frequency to %s minutes", new_stats_frequency_minutes, extra={"job": "stats"})
            except Exception as ex:
                logger.warning("Failed to reschedule Stats pipeline app config frequency: %s", str(ex), extra={"job": "stats"})
        else:
            logger.debug("Stats pipeline app config frequency has not changed (current freq: %s mins)", current_stats_frequency_minutes, extra={"job": "stats"})
    
    
    # ===== Inline async function for EMBEDDINGS PIPELINE =====
    async def embeddings_job():
        # 1. Get app-level config
        app_config = await app_config_repo.get_config()
        default_user_frequency_minutes = 1440


    # load appConfig from repo
    app_config = await app_config_repo.get_config()
    ingestion_pipeline_frequency_minutes = app_config.scheduler_config.ingestion_pipeline_frequency_minutes
    publishing_pipeline_frequency_minutes = app_config.scheduler_config.publishing_pipeline_frequency_minutes
    stats_pipeline_frequency_minutes = app_config.scheduler_config.stats_pipeline_frequency_minutes
    logger.info("Loaded DB app config: ingestion_freq=%s min, publishing_freq=%s min, stats_freq=%s min", ingestion_pipeline_frequency_minutes, publishing_pipeline_frequency_minutes, stats_pipeline_frequency_minutes)
    
    # setup job execution frequency
    scheduler.add_job(ingestion_job, "interval", minutes=ingestion_pipeline_frequency_minutes, id="ingestion_job")
    scheduler.add_job(publishing_job, "interval", minutes=publishing_pipeline_frequency_minutes, id="publishing_job")
    scheduler.add_job(stats_job, "interval", minutes=stats_pipeline_frequency_minutes, id="stats_job")

    # start scheduler.
    scheduler.start()
    logger.info("APScheduler started")

    yield  # Application runs here

    # shutdown scheduler
    scheduler.shutdown()
    logger.info("APScheduler stopped")


# Start FastAPI application
app = FastAPI(
    title       = "Ingestion and Publishing Pipelines",
    version     = "1.0.0",
    description = "",
    lifespan    = lifespan   # start the scheduler
)

# Register routes
app.include_router(pipeline_controller.router)


if __name__ == "__main__":
    # wrap ASGI server start-up under if __name__ == "__main__":, so the run doesnt double-execute
    uvicorn.run("main:app", host="0.0.0.0", port=8081)       # En PRO --> reload=False